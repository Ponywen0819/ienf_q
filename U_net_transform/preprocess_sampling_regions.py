import numpy as np
import cv2
from pathlib import Path
import pickle
import json
from tqdm import tqdm

def preprocess_sampling_regions(data_dir, output_dir=None):
    """
    é è™•ç†æ‰€æœ‰åœ–åƒï¼Œè¨ˆç®—æŠ½æ¨£ç¯„åœä¸¦ä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        data_dir: åŒ…å«æ‰€æœ‰åœ–åƒæ–‡ä»¶çš„ç›®éŒ„
        output_dir: è¼¸å‡ºé è™•ç†çµæœçš„ç›®éŒ„ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨data_dir
    """
    data_dir = Path(data_dir)
    if output_dir is None:
        output_dir = data_dir / "preprocessed"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(exist_ok=True)
    
    # æ‰¾åˆ°æ‰€æœ‰å®Œæ•´çš„åœ–åƒçµ„
    image_files = []
    for img_file in data_dir.glob("*.tif"):
        # è·³éå¸¶æœ‰å¾Œç¶´çš„æª”æ¡ˆ
        if any(suffix in img_file.name for suffix in ['_label', '_mask', '_boundary']):
            continue
            
        base_name = img_file.stem
        label_file = data_dir / f"{base_name}_label.tif"
        mask_file = data_dir / f"{base_name}_mask.tif"
        boundary_file = data_dir / f"{base_name}_boundary.tif"
        
        if label_file.exists() and mask_file.exists() and boundary_file.exists():
            image_files.append({
                'base_name': base_name,
                'image': img_file,
                'label': label_file,
                'mask': mask_file,
                'boundary': boundary_file
            })
    
    print(f"Found {len(image_files)} complete image sets")
    
    # è™•ç†æ¯å€‹åœ–åƒçµ„
    sampling_data = {}
    
    for file_info in tqdm(image_files, desc="Processing images"):
        base_name = file_info['base_name']
        
        try:
            # è®€å–åœ–åƒ
            label = cv2.imread(str(file_info['label']), cv2.IMREAD_GRAYSCALE)
            mask = cv2.imread(str(file_info['mask']), cv2.IMREAD_GRAYSCALE)
            boundary = cv2.imread(str(file_info['boundary']), cv2.IMREAD_GRAYSCALE)
            
            # è¨ˆç®—æŠ½æ¨£å€åŸŸ
            sampling_info = calculate_sampling_region(label, mask, boundary)
            
            # ä¿å­˜åˆ°å­—å…¸
            sampling_data[base_name] = sampling_info
            
            # å¦å¤–ä¿å­˜ç‚ºç¨ç«‹çš„ numpy æ–‡ä»¶ï¼ˆä¾¿æ–¼å¿«é€ŸåŠ è¼‰ï¼‰
            np.savez_compressed(
                output_dir / f"{base_name}_sampling.npz",
                **sampling_info
            )
            
        except Exception as e:
            print(f"Error processing {base_name}: {e}")
            continue
    
    # ä¿å­˜æ•´é«”çµ±è¨ˆä¿¡æ¯
    stats = calculate_dataset_statistics(sampling_data)
    
    # ä¿å­˜æ‰€æœ‰æŠ½æ¨£æ•¸æ“š
    with open(output_dir / "sampling_data.pkl", 'wb') as f:
        pickle.dump(sampling_data, f)
    
    # ä¿å­˜çµ±è¨ˆä¿¡æ¯ï¼ˆJSONæ ¼å¼ä¾¿æ–¼æŸ¥çœ‹ï¼‰
    with open(output_dir / "dataset_stats.json", 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"Preprocessing completed. Results saved to {output_dir}")
    print(f"Dataset statistics:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    return sampling_data, stats

def create_upper_region_mask(boundary, mask):
    """
    å‰µå»ºä¸­å¿ƒç·šä»¥ä¸Šçš„å€åŸŸé®ç½©
    
    Args:
        boundary: é‚Šç•Œåœ–åƒ (255 ç‚ºä¸­å¿ƒç·š)
        mask: åŸå§‹é®ç½©åœ–åƒ (255 ç‚ºæœ‰æ•ˆå€åŸŸ)
    
    Returns:
        upper_region: ä¸­å¿ƒç·šä»¥ä¸Šçš„æœ‰æ•ˆå€åŸŸé®ç½©
    """
    h, w = boundary.shape
    upper_region = np.zeros_like(mask)
    
    # æ‰¾åˆ°ä¸­å¿ƒç·šåƒç´ 
    boundary_pixels = (boundary == 255)
    
    # å°æ¯ä¸€åˆ—ï¼Œæ‰¾åˆ°æœ€ä¸Šæ–¹çš„é‚Šç•Œåƒç´ 
    for col in range(w):
        boundary_rows_in_col = np.where(boundary_pixels[:, col])[0]
        
        if len(boundary_rows_in_col) > 0:
            # ä½¿ç”¨æœ€ä¸Šæ–¹çš„é‚Šç•Œåƒç´ ä½œç‚ºè©²åˆ—çš„åˆ†ç•Œé»
            top_boundary = np.min(boundary_rows_in_col)
            # å°‡è©²åˆ†ç•Œé»ä»¥ä¸Šçš„åƒç´ æ¨™è¨˜ç‚ºæœ‰æ•ˆ
            upper_region[:top_boundary, col] = mask[:top_boundary, col]
        else:
            # å¦‚æœè©²åˆ—æ²’æœ‰é‚Šç•Œåƒç´ ï¼Œä½¿ç”¨æ•´åˆ—çš„ä¸ŠåŠéƒ¨åˆ†
            mid_point = h // 2
            upper_region[:mid_point, col] = mask[:mid_point, col]
    
    return upper_region

def calculate_sampling_region(label, mask, boundary):
    """
    è¨ˆç®—å–®å€‹åœ–åƒçš„æŠ½æ¨£å€åŸŸä¿¡æ¯ï¼ˆå‹•æ…‹æŠ½æ¨£ç‰ˆæœ¬ï¼‰
    åªä¿å­˜æœ‰æ•ˆæŠ½æ¨£ç¯„åœï¼Œä¸é å…ˆè¨ˆç®—åƒç´ åº§æ¨™
    
    Args:
        label: æ¨™ç±¤åœ–åƒ (255 ç‚ºç¥ç¶“äº®é»)
        mask: é®ç½©åœ–åƒ (255 ç‚ºæœ‰æ•ˆå€åŸŸ)  
        boundary: é‚Šç•Œåœ–åƒ (255 ç‚ºä¸­å¿ƒç·š)
    
    Returns:
        dict: åŒ…å«æŠ½æ¨£ç¯„åœä¿¡æ¯çš„å­—å…¸
    """
    h, w = label.shape
    
    # å‰µå»ºä¸­å¿ƒç·šä»¥ä¸Šçš„å€åŸŸé®ç½©
    upper_region = create_upper_region_mask(boundary, mask)
    
    # è¨ˆç®—çµ±è¨ˆä¿¡æ¯ï¼ˆç”¨æ–¼é©—è­‰å’Œçµ±è¨ˆï¼‰
    neural_count = np.sum(label == 255)
    non_neural_count = np.sum((label == 0) & (upper_region == 255))
    max_sample_count = min(neural_count, non_neural_count)
    boundary_pixel_count = np.sum(boundary == 255)
    
    # åªä¿å­˜ç”¨æ–¼å‹•æ…‹æŠ½æ¨£çš„å¿…è¦ä¿¡æ¯
    sampling_info = {
        'upper_region': upper_region.astype(np.uint8),
        'neural_count': int(neural_count),
        'non_neural_count': int(non_neural_count), 
        'max_sample_count': int(max_sample_count),
        'boundary_pixel_count': int(boundary_pixel_count),
        'image_shape': (h, w)
    }
    
    return sampling_info

def calculate_dataset_statistics(sampling_data):
    """è¨ˆç®—æ•´å€‹æ•¸æ“šé›†çš„çµ±è¨ˆä¿¡æ¯"""
    stats = {
        'total_images': len(sampling_data),
        'neural_counts': [],
        'non_neural_counts': [],
        'max_sample_counts': [],
        'boundary_pixel_counts': [],
        'upper_region_pixel_counts': [],
        'image_shapes': []
    }
    
    for base_name, info in sampling_data.items():
        stats['neural_counts'].append(info['neural_count'])
        stats['non_neural_counts'].append(info['non_neural_count'])
        stats['max_sample_counts'].append(info['max_sample_count'])
        stats['boundary_pixel_counts'].append(info['boundary_pixel_count'])
        stats['upper_region_pixel_counts'].append(np.sum(info['upper_region'] == 255))
        stats['image_shapes'].append(info['image_shape'])
    
    # è¨ˆç®—çµ±è¨ˆå€¼
    def calc_stats(values):
        return {
            'mean': float(np.mean(values)),
            'std': float(np.std(values)),
            'min': int(np.min(values)),
            'max': int(np.max(values)),
            'median': float(np.median(values))
        }
    
    summary_stats = {
        'total_images': stats['total_images'],
        'neural_count_stats': calc_stats(stats['neural_counts']),
        'non_neural_count_stats': calc_stats(stats['non_neural_counts']),
        'max_sample_count_stats': calc_stats(stats['max_sample_counts']),
        'boundary_pixel_count_stats': calc_stats(stats['boundary_pixel_counts']),
        'upper_region_pixel_count_stats': calc_stats(stats['upper_region_pixel_counts']),
        'common_image_shapes': list(set(stats['image_shapes'])),
        'sampling_feasibility': {
            'images_with_samples': sum(1 for count in stats['max_sample_counts'] if count > 0),
            'images_without_samples': sum(1 for count in stats['max_sample_counts'] if count == 0)
        }
    }
    
    return summary_stats

def load_sampling_data(preprocessed_dir, base_name):
    """
    åŠ è¼‰é è™•ç†çš„æŠ½æ¨£æ•¸æ“š
    
    Args:
        preprocessed_dir: é è™•ç†æ•¸æ“šç›®éŒ„
        base_name: åœ–åƒåŸºç¤åç¨±
    
    Returns:
        dict: æŠ½æ¨£ä¿¡æ¯
    """
    preprocessed_dir = Path(preprocessed_dir)
    
    # å˜—è©¦åŠ è¼‰ npz æ–‡ä»¶
    npz_file = preprocessed_dir / f"{base_name}_sampling.npz"
    if npz_file.exists():
        data = np.load(npz_file)
        return {key: data[key] for key in data.keys()}
    
    # å¦‚æœæ²’æœ‰ npz æ–‡ä»¶ï¼Œå˜—è©¦å¾ pickle æ–‡ä»¶åŠ è¼‰
    pkl_file = preprocessed_dir / "sampling_data.pkl"
    if pkl_file.exists():
        with open(pkl_file, 'rb') as f:
            sampling_data = pickle.load(f)
        return sampling_data.get(base_name, None)
    
    return None

def verify_preprocessing(data_dir, output_dir=None):
    """é©—è­‰é è™•ç†çµæœçš„æ­£ç¢ºæ€§"""
    data_dir = Path(data_dir)
    if output_dir is None:
        output_dir = data_dir / "preprocessed"
    else:
        output_dir = Path(output_dir)
    
    # éš¨æ©Ÿé¸æ“‡ä¸€å€‹åœ–åƒé€²è¡Œé©—è­‰
    all_tif_files = list(data_dir.glob("*.tif"))
    base_images = [f for f in all_tif_files if not any(suffix in f.name for suffix in ['_label', '_mask', '_boundary'])]
    sample_file = base_images[0] if base_images else None
    
    if sample_file is None:
        print("âŒ No valid image files found")
        return False
    base_name = sample_file.stem
    
    print(f"Verifying preprocessing for: {base_name}")
    
    # åŠ è¼‰åŸå§‹æ•¸æ“š
    label = cv2.imread(str(data_dir / f"{base_name}_label.tif"), cv2.IMREAD_GRAYSCALE)
    mask = cv2.imread(str(data_dir / f"{base_name}_mask.tif"), cv2.IMREAD_GRAYSCALE)
    boundary = cv2.imread(str(data_dir / f"{base_name}_boundary.tif"), cv2.IMREAD_GRAYSCALE)
    
    # é‡æ–°è¨ˆç®—
    fresh_calc = calculate_sampling_region(label, mask, boundary)
    
    # åŠ è¼‰é è™•ç†çµæœ
    loaded_data = load_sampling_data(output_dir, base_name)
    
    if loaded_data is None:
        print("âŒ Failed to load preprocessed data")
        return False
    
    # æ¯”è¼ƒçµæœï¼ˆå‹•æ…‹æŠ½æ¨£ç‰ˆæœ¬ï¼‰
    checks = [
        ('neural_count', fresh_calc['neural_count'] == loaded_data['neural_count']),
        ('non_neural_count', fresh_calc['non_neural_count'] == loaded_data['non_neural_count']),
        ('max_sample_count', fresh_calc['max_sample_count'] == loaded_data['max_sample_count']),
        ('boundary_pixel_count', fresh_calc['boundary_pixel_count'] == loaded_data['boundary_pixel_count']),
        ('upper_region', np.array_equal(fresh_calc['upper_region'], loaded_data['upper_region']))
    ]
    
    all_passed = True
    for check_name, passed in checks:
        status = "âœ…" if passed else "âŒ"
        print(f"{status} {check_name}: {passed}")
        if not passed:
            all_passed = False
    
    if all_passed:
        print("ğŸ‰ All verification checks passed!")
    else:
        print("âš ï¸  Some verification checks failed!")
    
    # é¡¯ç¤ºä¸€äº›çµ±è¨ˆä¿¡æ¯
    print(f"\nSample statistics:")
    print(f"  Neural pixels: {fresh_calc['neural_count']}")
    print(f"  Non-neural candidates: {fresh_calc['non_neural_count']}")
    print(f"  Max sample count: {fresh_calc['max_sample_count']}")
    print(f"  Boundary pixels: {fresh_calc['boundary_pixel_count']}")
    print(f"  Upper region pixels: {np.sum(fresh_calc['upper_region'] == 255)}")
    
    return all_passed

if __name__ == "__main__":
    data_dir = "/home/bl515-ml/Documents/shaio_jie/ienf_q/Centered"
    
    print("Starting preprocessing...")
    sampling_data, stats = preprocess_sampling_regions(data_dir)
    
    print("\nVerifying preprocessing results...")
    verify_preprocessing(data_dir)